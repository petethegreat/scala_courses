\chapter{parallel programming}

parallelism and concurrency

parallelism is doing several things at the same time, by efficently using hardware
parallelism is mainly concerned with organising the computational prbolem into multiple subproblems that can be executed simultaneously and independently.
parallel programming focusses on structuring the algorithms and/or data, such that as many numbers can be crunched as quickly as possible.

Bit-level parallelism is structuring the hardware/software so that a larger number of bits are involved in a single operation. Think of addition - one bit addition would add the ones bits, then carry over (if needed) and add the twos bits, then carry over and do the fours bits, and so on. Alternatively, a four bit algorithm/hardware could add all four of these bits simultaneously in a single operation.

Instruction level parallelism is executing different instructions from the same instruction stream in parallel

concurrency is where multiple instructions may or may not execute at the same time.
Main focus of conncurrency is figuring out how to structure a program such that some parts can continue to execute while other parts wait for things (fetch data from memory/disk/database/web). Concurrent programming is targeted towards writing asynchrounous applications.

There is some overlap between the two, but neither is a superset of the other.


c = a2 + a2
b = a1 + a2
d = b + c

b and c can be computed in parallel. This is instruction level parallelism

task level parallism - carrying out operations from two (or more) seperate instruction streams at the same time.
THis is what we deal with mostly.

\section{paralleism in the jvm}

will assume we are running on a multicore/multiprocessor system

\begin{itemize}
    \item OS - software that manages hardware and software resources, and schedules program execution
    \item process - an instance of a program
    \begin{itemize}
        \item The same program can be started more than once, and thus may be running in multiple processes
    \end{itemize}
    \item threads

\end{itemize}

There are usually many processes running on a small number of cpus. The OS schedules execution - each process will run for a small chunk of time ( {\em time slice}), then the os switches execution to another. This is multitasking.

two different processes cannot directly access each other's memory - they are isolated.

A single program (process) may make use of multiple threads (concurrency units). All threads within a program have access to the program's memory space.

Each thread has a program counter and a program stack. A given thread may not access the stack (or stack variables) of another thread. For threads to communicate, they must read/write to shared (heap) memory.

\section{threading in the jvm}

\begin{itemize}
    \item define a subclass of \lstinline|Thread|
    \item instantiate an object of this subclass
    \item invoke the \lstinline|start| method of the subclass
\end{itemize}

The definition of the subclass defines the code that will be used. 

An operation is {\em atomic} if it occurs instantaenously from the perspective of other threads.
The \lstinline|synchronized| block can be invoked to ensure atomicity. No more than one thread may execute the same synchronized block at the same time.

synchronized blocks can be nested. For example, when transferring from a source to a destination, you might want to invoke synchoronised on both objects, such that the transfer is atomic.

\section{deadlock}
Deadlock occurs when two (or more) threads wait to acquire resources without releasing resources that they hold. None of the threads can proceed, as all are waiting for another thread to finish. This can happen when \lstinline|synchronized| blocks are nested.

Deadlocks can be avoided by always acquiring resources in a specific order. In the transfer example, each account is associated with a unique id. For a given transfer, the lowest uid is acquired first, followed by the other. This avoids circular dependencies, as things hold the lowest and wait for the highest. Assuming self-transfers are guarded against, then one of the threads involving the highest account will be able to complete, freeing the lower, allowing other threads to proceed. 

\section{memory model}
the memory model is a set of rules used in the JVM that describes how threads will interact when accessing shared memory.

\begin{itemize}
    \item two threads writing to seperate locations do not require synchronisation
    \item A thread X that joins on thread Y is guaranteed to observe all writes performed by Y after join returns.
\end{itemize}

Threads and Synchronization are fairly low level functionalities. In practice we will probably work with higher level abstractions.


\section{running computations in parallel}

Consider the case where we want to compute the pnorm for an array

\begin{equation}
\norm{\mathbf{x}}_p = \left(\sum_{i=1}^{n} |x_i|^p \right)^{1/p}
\end{equation}


Simplest case is to just sequentially loop over all the elements.
We can use two threads. Define $m=n/2$, then one thread computes the sum for $1 \leq i < m$, the other for $m \leq i \leq n$. Then we add the results and raise to the power $1/p$.

Following will compute the sum of the raised elements of an array (worksheet pnorm)
{\bf Note} - the ending index on slice is exclusive.
\begin{lstlisting}
def sumSegment(a: Array[Int], p:Double, s: Int, t:Int): Int =
  {
    def raise(aa:Int): Int = floor(pow(aa.abs,p)).toInt

    a.slice(s,t).map(raise).sum
  }

val myArray = Array(1,2,3,4,5,6)

sumSegment(myArray,2,0,3)
\end{lstlisting}

Four four threads, we could similarly divide the initial array into four pieces. What if we have an unbounded number of threads? If there is no predefined limit on the number of threads we may use, then we can break this into as many pieces as is suitable. There is some overhead associated with setting up threads, so we define some threshold. For a (sub) array with size less than the threshold, we just operate on it sequentially, else we break our array into two pieces and parallelise across these.

\begin{lstlisting}

def pNormRec(a: Array[Int],p:Double): Int = 
    power(segmentRec(a,p,0,a.length),1/p)

def segmentRec(a: Array[Int],p: Double,s: Int,t: Int) = {
    if (t-s < threshold)
        sumSegment(a,p,s,t)
    else
    {
    val m = s + (t - s)/2 // (t +s) /2 ?
    val sum1, sum2 = parallel(
        segmentRec(a,p,s,m),
        segmentRec(a,p,m,t))
    sum1 +sum2
    }

}
\end{lstlisting}
Again, note that as the ending index on slice is exclusive, we can specify $m$ as both the end index for the first segment and the start index for the second segment.

\section{first class tasks}
Previously considered a construct, parallel, that would evaluate expressions in parallel
\begin{lstlisting}
val (v1,v2) = parallel(e1,e2)
\end{lstlisting}

Alternatively, we could use a task construct.
\begin{lstlisting}

val t1 = task(e1) // spawns a new thread computing e1
val t2 = task(e2) // spawns a new thread computing e2
val v1 = t1.join // joins t1, obtaining result v1 (waits till t1 thread completes)
val v2 = t2.join // joins t2 with result v2

\end{lstlisting}

When creating the tasks, the current computation (presumably the main thread) continues. The join calls are { \em blocking} - the main thread above will wait until task t1 concludes. Subsequent calls to t1.join quickly return the same result.


parallel can be expressed in terms of task
\begin{lstlisting}
def parallel[A,B](cA: => A, cB: => B): (A,B) = {
    //val ta = task cA
    val tb = task cB
    //(ta.join, tb.join)
    val aa = cA
    (aa,tb.join)

    }

\end{lstlisting}


Note that the "task" construct that is mentioned here is implemented in the assignment. The task and parallel implemtations wrap functionality from \lstinline|java.util.concurrent| (ForkJoinTask).

\section{complexity of parallel programs}
W(e) - the work of a sequential program (single thread)
treat parallel as pair - parallel(e1,e2) -> (e1,e2)
D(e) - the depth of a parallel program (or span) - the number of steps it would be broken into given unbounded parallelism.

W(parallel(e1,e2)) = W(e1) + W(e2) + c1
D(parallel(e1,e2)) = max(D(e1),D(e2)) + c2

If work is divided into equal parts, for depth we only count one part.
for parts where we do not explicitly call parallel, we add the constituents

Suppose our platform has P processors. The fastest we could do all the work is W(e)/P
Suppose we had unbounded parallelism (infinite threads). The fastest we could do everything is D(e)

D(e) + W(e)/P is often used as an estimate for running time. Given W and D, we can estimate how performance time will behave for different P. If P is constant but inputs grow, then the running time scales linearly with the size of the inputs.
Even with infinite resources, we are still bounded by D(e).

\subsection{Amahl's law}

Suppose a program has two parts. part $p_1$ takes 40\% of the time, while part $p_2$ takes the reamining 60\% of the time, but can be sped up. If we make part 2 faster by $P$ times, the speedup for the entire program is

\begin{equation}
S = \frac{1}{f + \frac{1-f}{p}}
\end{equation}

which tends to $1/f$ as $P \rightarrow \infty$.


\section{parallel sorting}

\subsection{Merge sort}
This is interesting, as it's not a pure merge sort. Paralleisation is a function of the depth at which we're operating - if we are up high, then we split the array and create new threads. Once a sufficient depth has been reached, we stop spawning new threads. However, instead of doing a mergesort sequentially, we change the algorithm to a quickSort, which operates sequentially.

Sorting is done in place, but auxillary storage is used for merging. At each depth level, we alternate between merging from xs into ys with merging from ys into xs.

\begin{lstlisting}

def parMergeSort(xs: Array[Int, maxDepth:Int): Unit = {
    // aux storage
    val ys = new Array[Int](xs.length)

    def sort(from: Int, until: Int, depth:Int) = {
        if (depth == maxDepth) {
            quickSort(xs, from, until - from)
        } else {
            val mid = (from + until)/2
            parallel(sort(from,mid,depth+1),sort(mid,until,depth+1))

            val flip = depth %2 == 0
            val src = if (flip) ys else xs
            val dst = if (flip) xs else ys
            merge(src,dst,from,mid,until)
        }
    }
}
\end{lstlisting}

\subsection{parrallel operations on collections}

operations on collections (such as map, fold, scan) are pretty foundational to functional programming.

scan 
List(1,3,8).scan(100)((s,x) => s + x ) == List(100, 101, 104, 112)

like zip + fold? can think of scan as applying fold to all list prefixes, or storing the intermediate results of fold

List is not an ideal data structure for parallel computations, as we need to search through the list to find the middle. concatenation also takes linear time.

\subsection{Fold Operations and Associativity}

Map applies an operation to each element in the sequence. Fold uses a reduction operator (and an initial value) to combine elements, returning a single result. FoldLeft and FoldRight start at the beginning and end of the sequence, respectively. 

The result of a fold operation can depend on the order in which the operation is applied to the collection. 
\begin{lstlisting}
List(1,3,8).foldLeft(100)((s,x) => s - x) == (((100 -1) -3) -8) == 88
List(1,3,8).foldRight(100)((s,x) => s - x) == (1- (3 - (8 - 100))) == -94
\end{lstlisting}

To enable parallel operations on collections, we consider associative operations - the order of the operations does not matter
\begin{eqnarray*}
f(a,f(b,c)) & = & f(f(a,b),c) \\
1 + (2 + 3) & = & (1 + 2) + 3 \\
1 - (2 -3) & \neq & (1 -2) -3 \\
\end{eqnarray*}
Addition (and multiplication) are associative, whereas subtraction and division are not. Multiplication is commutative also, in that $ f(a,b) = f(b,a)$, whereas division and subtraction are not. String concatenation is also associative.

The operation $f(a,b)$ can be written in infix form $ a \otimes b$. A sequence of these operations can be written in the form of a tree, where the leaves are values and nodes are $\otimes$. If the operation is associative, then two expressions with the same list of operands, but parentheses placed in different locations, will evaluate to the same result. 

A reduce operation on any tree with this list of operands will yield the same result.

\subsubsection{commutative but not associative} Think of the operation that yields the winning move in a game of rock, paper, scissors, $r \otimes p = p$. As $p \otimes r = p$, this operation commutes. but $ (r \otimes s) \otimes p = r \otimes p = p$, whereas $r \otimes (s \otimes p) = r \otimes s = r$, so the operation is not associative.

\subsubsection{associative but not commutative}
\begin{itemize}
    \item concatenation of strings, lists
    \item matrix multiplication
    \item composition of relations
    \item composition of functions 
\end{itemize}

Associativity is not always preserved by mapping. When combining and optimising map/reduce operations, we need to be careful that the operations given to reduce remain associative (otherwise the result depends on the order in which computations run, which may be more or less random).

Floating point addition and multiplication may not be associative, due to precision/rounding errors.

\subsection{parallel scan and prefix sums}

Scan is an operation that  reduces a collection, but returns the intermediate results of the reduction. Instead of just keeping track of the total (the final result), the cumulative results of the reduction are stored in an array.

\begin{lstlisting}
def scanLeft[A](inp: Array[A], a0: A, f: (A,A) => A, out: Array[A]): Unit = {
    
    out(0) = 0
    var a = a0
    var i = 0
    while (i < inp.length) {
        a = f(a, inp(i))
        i = i+1
        out(i) = a
    }
}
\end{lstlisting}

Parallelising scanLeft seems like a problem, as ordering is important. We assume that each element of the output requires the preceding element to be computed. This isn't strictly true, We can have pieces run independantly if we give up on using the intermediate results. We call the reduction function f many more times, but the gains from parallelism make up for this.

\begin{lstlisting}
def reduceSeg1[A](inp: Array[A], left: Int, right: Int, a0:A, f: (A,A) => A ) :A = {
    
}
def mapSeg[A,B] (inp: Array[A], left: Int, right: Int, fi: (int,A) => B, out: Array[B] ) :Unit = {}


def scanLeft[A](inp: Array[A], a0: A, f: (A,A) => A, out: Array[A]): Unit = {
val fi = { (i: Int, v: A) => reduceSeg1(inp1,0,i,a0,f)} // like currying? fi is a function
mapSeg(inp,0,inp.length,fi,out)
val last = inp1.length -1
out(last+1) = f(out(last),inp(last))
}
\end{lstlisting}

This implementation does not keep track of "the preceding element". \lstinline|fi| is a function that takes an index $i$ and carries out the mapping (of \lstinline|f| on \lstinline|inp|) for the first $i$ elements of \lstinline|inp|. \lstinline|mapSeq| carries out the mapping of \lstinline|fi| on \lstinline|inp|. Lots of computation could be saved by using an intermediate/output array for storage, but there is io associated with this. if the mapping function f is fast, or if things can be implemented tail recursively, then the additional computation (running in parallel) may be faster than the sequential version. In fact, as the parallelised map has $\mathrm{log}(n)$ complexity, the overall complexity of the scan is $\mathrm{log|(n)}$.

Can use trees to keep track of what has already been computed, so we are not doing the same thing over and over (and over).

define a couple of tree structures:
\begin{lstlisting}
// data structure for our initial input collection
sealed abstract class Tree[A]
case class Leaf[A] (val a:A) extends Tree[A]
case class Node[A] (l: Tree[A], r: Tree[A]) extends Tree[A]

// result tree, here nodes have values also, not just leafs
sealed abstract class TreeRes[A](val res:A)
case class LeafRes[A] (override val res:A) extends TreeRes[A]
case class NodeRes[A] (l: TreeRes[A], override val res:A, r: TreeRes[A], ) extends TreeRes[A]

// reduction that preserves computation tree
// sequential
def reduceRes[A](t :Tree[A], f: (A,A) => A): TreeRes[A] = t match {
    case Leaf(v): LeafRes(v)
    case Node(l,r): {
        val (tL,tR) = (reduceRes(l,f),reduceRes(r,f))
        NodeRes(tL, f(tL.res,tR.res),tR)
        }
    }
\end{lstlisting}

We can easily parallelise this. The function upsweep carries this out (bottom-up computation of the result tree). For scans, would like to move through the result tree in a particular order. In Downsweep, we move through the result tree, and output a tree containing the scan results.

\begin{lstlisting}
def upsweep(t: Tree[A],f: (A,A) => A) = t match {
    case Leaf(v): LeafRes(v)
    case Node(l,r): {
        val (tL,tR) = parallel(upsweep(l,f),upsweep(r,f))
        NodeRes(tL, f(tL.res,tR.res),tR)
        }
    }
//a0 is the reduce of all elements to the left
def downsweep(t: Tree[A], a0: A, f: (A,A) => A):Tree[A] = t match {
    case LeafRes(a): Leaf(f(a0),a)
    case NodeRes(l,a,r): {
        val (tL,tR) = parallel(downsweep[A](l,a0,f),downsweep[A](r,f(a0,l.res),f))
        Node(tL,tR)
        }
    }

\end{lstlisting}

To do scanleft, we build the result tree (with upsweep), then downsweep.
\begin{lstlisting}
def scanLeft[A](t: Tree[A], f: (A,A) =>A): Tree[A] = {
    val tRes = upsweep(t,f)
    val scanl = downsweep(tRes,a0,f)
    prepend(a0,scanl)
}

def prepend[A](x: A, t: Tree[A]) = t match {
    case Node(l,r): Node(prepend[A](x,l))
    case Leaf(v): Node(Leaf(x), Leaf(v))

}
\end{lstlisting}

Extend this to work with collections that are initially arrays. Will still use a tree for the intermediate results (because trees are awesome). We don't keep track of array elements, instead we track ranges and pass around a reference to the array.

\begin{lstlisting}
sealed abstract class TreeRes[A](val res:A)
case class LeafRes[A] (from: Int, to: Int, override val res:A) extends TreeRes[A]
case class NodeRes[A] (l: TreeRes[A], override val res:A, r: TreeRes[A], ) extends TreeRes[A]

def upsweep(inp: Array[A], from: Int, to: Int, f: (A,A) => A): TreeRes[A] = {
    if (to - from < threshold)
        LeafRes(from,to,reduceSeg1(inp, from+1, to, inp(from),f))
    else {
        val mid = from + (from - to)/2
        val (tL,tR) = parallel(upsweep(inp,from,mid,f),upsweep(inp,mid,to,f))
        NodeRes(tL,f(tL.res,tR.res),tR)}
    }

}
\end{lstlisting}
\chapter{Data Parallelism}
Task parallelism is a form of parallisation that distributes execution processes across computing nodes.
data parallelism is a form of parallelism that distributes data across copmuting nodes.

\begin{lstlisting}
def initializeArray(xs: Array[Int])(v: Int): Unit = {
    for (i <- (0 until xs.length).par) {
    xs(i) = v
    }
}
\end{lstlisting}

The par makes use of scala parallel collections.

The amount of processing required by each element of data is the workload. In the case of initializeArray, the workload is uniform, which means that the data-parallel processing works well (all elements take the same time, good speedup, linear performance).
For the case of the Mandelbrot set processing, some pixels take more iterations than others (pixels in the set compared to those that are determined to be out of the set almost immediately). This means that the workload is unbalanced/irregular.

\section{parallel collections}

the \lstinline|.par| call converts a sequential collection into a parallel collection.
\begin{lstlisting}
(1 untill 1000).par
.filter(n => n%3 ==0)
.count(n => n.toString == n.toString.reverse)
\end{lstlisting}

Not everything can be parallelised. FoldLeft requires that the reduction operation be carried out in a specific order (as the collection's element type and the reduction type may be dfferent). For the fold operation, the result type is the same as the element type, so there are mulitple ways in which the element-wise reductions can be ordered. This can be parallelised, as different processes can reduce different chunks of the data, and then the "subtotals" may be merged.


\subsection{fold}
For fold to work consistently on a parallel collection, there are a couple of conditions that must be satisfied:
\begin{itemize}
    \item the folding operation must be associative - \lstinline|f(a, f(b,c)) == f(f(a,b),c)|
    \item the neutral element must be ignored - \lstinline|f(a,z) == f(z,a) == a|
\end{itemize}
We say that the neutral element and the function $f$ form a { \em monoid}.

The signature of of the function given to fold must be of the form \lstinline| f: (A,A) => A |, with the neutral element of type \lstinline|A|. This is fairly rigid. foldLeft was more flexible, but is not assciative \lstinline|f: (B,A) => B|, where A is the type of the collection element and B is the type of the reduction.

Aggregate is like fold, but is more flexible. It takes a neutral element, and two combination operations, one for combinig two subtotals, and one for adding a collection element to a subtotal.
collection.par.aggregate

\section{collections - hierarchy review}

\begin{itemize}
    \item \lstinline|Traversable[T]| - collection of elements of type T, with operations implemented using foreach
    \item \lstinline|Iterable[T]| - collection of elements of type T, with operations implemented using iterator
    \item \lstinline|Seq[T]| an ordered sequence of elements of type T
    \item \lstinline|Set[T]| a set of elements of type T, no duplicates.
    \item \lstinline|Map[K,V]| a mapping from keys of type K to values of type V, no duplicate keys.
\end{itemize}

\lstinline|ParIterable, ParSet, ParMap, ParSeq| are the parallel analogs of the above.
Generic collections \lstinline|GenIterable, GenSet, GenMap, GenSeq| are types for code that is agnostic about parallelism.

Generic collections allow us to write code that is unaware of parallelism. {\bf These were apparently deprecated in later versions of scala.}

Need to use parallel collections with care:
\begin{itemize}
    \item Never read from a parallel collection that is concurrently modified.
    \item Never write to a parallel collection that is concurrently traversed
\end{itemize}
TrieMap is an exception, can create a snapshot of the current map state, and then update the map based off the snapshot.

\section{ builders and combiners}
transformer operations take collections and return collections (map, flatmap, filter, groupBy)
fold, sub, aggregate are not transformer operations

Builder is a trait that is used in sequential collection methods
Elements can be added to the builder, and when the result method is called it returns the desired collection.

Parallel collections cannot use builders, but instead can make use of combiners. Multiple combiners can be combined.

When the collection (repr) is a set or map, combine represents the union.
when the collection is a sequence, combine represents concatenation.

In order for combiners to be efficient, the combine operation must have complexity $O( \log n + \log m)$, where $n$ and $m$ are the size of the component combiners. 



