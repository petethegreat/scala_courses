\chapter{Big Data Analysis with Scala and Spark}

\section{Introduction}
Spark api for working with distributed data corresponds (almost) 1 to 1 with scalas api for working with collections

hyperref[mastering apache spark 2 - Jacek Laskowski]{https://books.japila.pl/apache-spark-internals/apache-spark-internals/index.html}

the books "high performance spark" and "advanced analytics with spark" are also recommeded.


\section{data parallel to distributed data parallelism}

\begin{itemize}
	\item data is in a collection
	\item data is split into shards
	\item workers/threads operate on shards in parallel
	\item results are combined when done (if neccasary)
\end{itemize}

A lot of this is just done under the hood, using the parallel collections api.

extending to multiple nodes:
\begin{itemize}
	\item data is in a collection
	\item data is split over several nodes
	\item nodes independantly work on data shards in parallel
	\item results are combined when done (if neccasary)
\end{itemize}

\section{Latency}


some operations (such as shuffles) have a much higher latency than others due to network communication (i/o). Shuffles. 


\begin{table}[hbp]
\small
\begin{tabular}{l r r r l}
\multicolumn{5}{c}{Latency Comparison Numbers ($\sim$2012)} \\
\hline\hline
L1 cache reference                 &          0.5 ns &             &         &  \\
Branch mispredict                  &          5   ns &             &         &  \\
L2 cache reference                 &          7   ns &             &         & 14x L1 cache \\
Mutex lock/unlock                  &         25   ns &             &         &  \\
Main memory reference              &        100   ns &             &         & 20x L2 cache, 200x L1 cache \\
Compress 1K bytes with Zippy       &      3,000   ns &        3 us &         & \\
Send 1K bytes over 1 Gbps network  &     10,000   ns &       10 us &         & \\
Read 4K randomly from SSD*         &    150,000   ns &      150 us &         & $\sim$1GB/sec SSD \\
Read 1 MB sequentially from memory &    250,000   ns &      250 us &         & \\
Round trip within same datacenter  &    500,000   ns &      500 us &         & \\
Read 1 MB sequentially from SSD*   &  1,000,000   ns &    1,000 us &    1 ms &  $\sim$1GB/sec SSD, 4X memory \\
Disk seek                          & 10,000,000   ns &   10,000 us &   10 ms &  20x datacenter roundtrip \\
Read 1 MB sequentially from disk   & 20,000,000   ns &   20,000 us &   20 ms &  80x memory, 20X SSD \\
Send packet CA->Netherlands->CA    &150,000,000   ns &  150,000 us &  150 ms &  \\

\end{tabular}
\caption{originally by Jeff Dean and Peter Norvig. Taken from \hyperref[here]{https://gist.github.com/jboner/2841832}}
\end{table}

Things involving network are {\textbf slow}. 

spark minimizes network traffic by keeping all data immutable and in memory. All operations are functional transformations. Fault tolerance is acheived by reapplying the set of transformations to the original dataset (or partition).

\section{RDDs}

Rdds implement map, flatmap, filter, and reduce. The api is much the same as lists, or other sequential or parallel collections. Fold and aggregate also. Aggregate is slightly different, in that for rdds it is call by value, whereas with scala collections it is call by name 

\subsection{transformations and actions}

scala has transformers and accessors. Transformers return new collections as results (instead of single values): map, flatmap, filter, groupBy.

accessors return single values: reduce, fold, aggregate.

in spark, transformations return new rdds, actions compute a result (either saved or written to storage).

transformations are lazy, actions are eager.

Anything that returns a value that is not an RDD is an action, and will be eagerly executed.

\subsection{evaluation}

\subsection{cluster topology}

driver vs worker.

\subsection{reduction operations}

fold, reduce, aggregate walk through the collection, and combine neighbouring elements to produce a single result.

As mentioned earlier, foldleft is not paralisable as the types change - we can aggregate partitions individually, but foldleft doesn't have an operation defined to reduce/merge across partitions. If our foldLeft operation is \lstinline| f(a:A,b:A) :B| (takes two inputs of type A and returns one of type B), each partition could be reduced, but the results of these are of type B, and we do not have a \lstinline| f(a:B,b:B) :B| operation. Aggregate solves this, as there it takes arguments for a reduction operation as well as a combination operation.

\subsecton{pair RDDs}
